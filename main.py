# app.py

import streamlit as st
import torch 

from PIL import Image

# Onglets pour chaque mod√®le
tab1, tab2, tab3 = st.tabs(["üñºÔ∏è Stable Diffusion", "üß† GPT-Neo", "ü¶ô LLaMA"])

# --- Onglet 1 : Stable Diffusion ---
with tab1:
    from diffusers import DiffusionPipeline

    st.title("üé® G√©n√©rateur d'image avec Stable Diffusion")
    prompt_sd = st.text_input("Entrez votre prompt pour l'image :", "a ball")

    if st.button("G√©n√©rer l'image", key="sd"):
        with st.spinner("‚è≥ G√©n√©ration de l'image..."):
            pipe = DiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4")
            pipe.to("cuda" if torch.cuda.is_available() else "cpu")
            image = pipe(prompt_sd).images[0]
            st.image(image, caption=f"R√©sultat pour : {prompt_sd}", use_column_width=True)

# --- Onglet 2 : EleutherAI GPT-Neo ---
with tab2:
    from transformers import pipeline as hf_pipeline

    st.title("üß† G√©n√©rateur de texte avec EleutherAI GPT-Neo")
    prompt_gpt = st.text_area("Entrez le contexte :", "The following text is a description of a man:")

    if st.button("G√©n√©rer le texte", key="gpt"):
        with st.spinner("‚è≥ G√©n√©ration en cours..."):
            generator = hf_pipeline("text-generation", model="EleutherAI/gpt-neo-1.3B")
            output = generator(prompt_gpt, max_length=150, num_return_sequences=1)[0]['generated_text']
            st.text_area("Texte g√©n√©r√© :", output, height=200)

# --- Onglet 3 : LLaMA ---
with tab3:
    st.title("ü¶ô G√©n√©rateur de texte avec LLaMA")
    prompt_llama = st.text_area("Entrez le d√©but de texte :", "Plants create energy through a process known as")

    if st.button("G√©n√©rer le texte", key="llama"):
        with st.spinner("‚è≥ G√©n√©ration avec LLaMA..."):
            llama_pipeline = hf_pipeline(
                task="text-generation",
                model="huggyllama/llama-7b",
                torch_dtype=torch.float16,
                device=0 if torch.cuda.is_available() else -1  # N√©cessite CUDA
            )
            output = llama_pipeline(prompt_llama, max_length=150, num_return_sequences=1)[0]['generated_text']
            st.text_area("Texte g√©n√©r√© :", output, height=200)
